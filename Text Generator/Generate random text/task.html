<h2><strong style="background-color: transparent; color: #000000; font-size: 11pt; font-variant: normal;"></strong>Theory</h2>

<p>We suggest using the method <a target="_blank" href="https://pynative.com/python-weighted-random-choices-with-probability/" rel="noopener noreferrer nofollow">random.choices()</a> to select the most probable tail from the list of possible tails based on the corresponding repetition counts. This method is similar to <code class="java">random.choice()</code> with the exception that it also considers user-specified weights during the process. The method takes four arguments: <code class="java">population</code>, <code class="java">weights</code>, <code class="java">cum_weights</code>, and <code class="java">k</code>. For this project, we only care about the first two arguments: <code class="java">population</code>, which is a list of elements to choose from, and <code class="java">weights</code>, which is a list of relative weights that correspond to the elements of the population. Since the other two arguments have sensible default values, we don't necessarily have to specify them. </p>

<h2>Description</h2>

<p>We have our model, fantastic! What's next? Well, the model can already be used to predict the next word in a chain by feeding it any head (of a bigram) from the corpus and retrieving the most probable tail from the corresponding entry. But how do we start the chain, what should be the first word?</p>

<p>Of course, we could choose a word manually, but this is an error-prone solution because we might take a word that is not in the corpus. A better way to start is to choose a random word from the corpus and feed it to the model so that it predicts the next word. <br>
After the next word is acquired, it should be used to predict the following word, and so on, thus continuing the chain.</p>

<h2>Objectives</h2>

<ol>
	<li>Choose a random word from the corpus that will serve as the first word of the chain.</li>
	<li>The second word should be predicted by looking up the first word of the chain in the model and choosing the most probable next word from the set of possible follow-ups. Right now, an entry contains all the possible tails that might follow the selected head along with their corresponding repetition counts. Using the repetition counts, you will be able to choose the most probable option.</li>
	<li>The second step should be repeated until the length of the chain is 10 words, but this time, the current last word of the chain should be used to look up another probable word to continue the sentence.</li>
</ol>

<p>Using the algorithm described above, generate chains consisting of 10 tokens, join the resulting tokens together, and print them as a pseudo-sentence. Keep in mind that a pseudo-sentence can consist of multiple actual sentences, so having punctuation marks inside pseudo-sentences is completely valid.</p>

<p>Generate and print 10 sentences like that. Keep in mind that every generated pseudo-sentence should be on a new line.</p>

<p>You should only print the output of the current stage and not the previous one. The name of the file that contains the corpus should be given as a command line input.</p>

<h2>Example</h2>

<p>The greater-than symbol followed by a space (<code class="java">&gt; </code>) represents user input.</p>

<p>The output of your program should have the same formatting style.</p>

<pre><code class="language-no-highlight">&gt; corpus.txt
so I saw him grow up against me halfway out
Queen of the night ashore for-- water. The Lannister song?
honor for all reading about me? Can't. Someone appears to
she would be easier than I sliced me, My atonement?
your days. Robert's return. A mountain of Casterly Rock. Has
much do me roar! For the King in the Kingslayer?
the side my pride. Don't lose. Have you were you
for you out there will take it You don't know.
she crucified the rest of them. The Boltons, the Watch
father Tywin sent here in their minds aren't they lick</code></pre>