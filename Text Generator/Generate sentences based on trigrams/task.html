<h2>Description</h2>

<p>The generated text finally looks like actual sentences! It's a great step in the right direction, but do they actually make sense?Unfortunately, the algorithm most likely generates gibberish that can hardly be called a product of an artificial "intelligence". This happens because the algorithm considers nothing but the preceding word when trying to predict the next one in the chain. It basically works like a person who forgets the beginning of their sentence while speaking and pronounces the first probable word that comes into their mind without really connecting it to the original idea of the sentence.</p>

<p>To improve the quality of the generated sentences, we need to take into account the longer bits of the preceding text. This is not an easy task!</p>

<h2>Objectives</h2>

<p>Right now, the model is based on bigrams, that is, we only consider one word when trying to predict the next word in the chain.</p>

<p>The algorithm should be extended so that it can use not only bigrams but also trigrams. We have already talked about trigrams in Stage 2. They are very similar to bigrams, the only difference being their length: trigrams consist of three tokens instead of just two. In the case of trigrams, every head should consist of two tokens. Tails should stay the same length as before since we are still aiming to predict the next word in the chain.</p>

<p>This change implies the following tasks:</p>

<ol>
	<li>The list of bigrams should be transformed into a list of trigrams. It should still consist of heads and tails, but now, heads should consist of two space-separated tokens concatenated into a single string. The tails should still consist of one token. For example: <em>head</em> — <code class="java">winter is</code>, <em>tail</em> — <code class="java">coming</code>.</li>
	<li>The model should be trained based on the list of trigrams. The model creation requires no modifications since trigrams still consist of a head and a tail.</li>
	<li>The beginning of the chain should be a randomly chosen head from the model, not just any word from the corpus.</li>
	<li>When predicting the next word, the model should be fed the concatenation of the last two tokens of the chain separated by a space.</li>
</ol>

<p>After making all these modifications, the output should look rather similar to the result of the previous stage, but now the generated pseudo-sentences should make a little more sense.</p>

<p>Keep in mind that every generated pseudo-sentence should be in a new line.</p>

<p>You should only print the output of the current stage and not the previous one. The name of the file that contains the corpus should be given as user input.</p>

<h2>Example</h2>

<p>The greater-than symbol followed by a space (<code class="java">&gt; </code>) represents user input.</p>

<p>The output of your program should have the same formatting as shown below.</p>

<pre><code class="language-no-highlight">&gt; corpus.txt
I sent men over the Wall every night.
Kill him! Kill all who understand the law.
They say 1,000 slaves died building the Great Keep at Winterfell.
Queen Margaery. She walked in on Craster's Keep on the Iron Throne.
They say 1,000 slaves died building the Great Keep at Winterfell.
And why is the wheel our queen when she needed me the most.
Dothraki omens. I waited 17 years ago there came a night with no regrets.
Ah, yes. You shall now be held accountable.
Don't cry. It will all be for you.
Never understood why some knights felt the tears freeze on their own.

</code></pre>